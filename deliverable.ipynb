{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ae122a",
   "metadata": {},
   "source": [
    "- **Project Title:** *Explainable Recommendations with BigQuery AI: From Profiles to Insights*\n",
    "- **Problem Statement:** Most recommendation systems function as “black boxes,” surfacing items without offering context or transparency. This project uses BigQuery AI to automatically generate structured profiles of users and items, transform them into embeddings, and apply vector search to retrieve the most relevant matches. Each recommendation is then paired with an LLM-generated explanation, providing narrative reasoning that makes the system’s suggestions interpretable and trustworthy.\n",
    "- **Impact Statement:** This solution shows how organizations can build scalable, explainable recommendation engines directly within BigQuery without the need for specialized machine learning teams. By combining personalization with transparency, the approach improves user trust, helps businesses uncover actionable behavioral patterns, and extends seamlessly to domains such as e-commerce and media.\n",
    "- **Project Highlights**\n",
    "    - Developed adaptable, structured profiles for users and recipes using BigQuery AI, enabling easy customization for various business domains.\n",
    "    - Demonstrated that a straightforward recommendation system powered by BigQuery AI can slightly outperform the ALS baseline, simplifying implementation and highlighting the value of LLMs in recommendation workflows.\n",
    "    - Created an interactive UI for users to browse personalized recommendations alongside clear, LLM-generated explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622911ca",
   "metadata": {},
   "source": [
    "# User & Recipe Profiles with BigQuery AI: From Embeddings to \"Explainable\" Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9356df23",
   "metadata": {},
   "source": [
    "\n",
    "Inspired by the DoorDash blog post on [Profile Generation with LLMs](https://careersatdoordash.com/blog/doordash-profile-generation-llms-understanding-consumers-merchants-and-items/), this notebook showcases how to build user and recipe profiles leveraging BigQuery AI. It also outlines a workflow for generating potential recommendations, each paired with an \"LLM-generated hypothesis\" to provide narrative context for the suggestions.\n",
    "\n",
    "This project addresses the challenges of **Approach 1 (The AI Architect)** and **Approach 2 (The Semantic Detective)** as described in the [Kaggle BigQuery Hackathon 2025](https://www.kaggle.com/competitions/bigquery-ai-hackathon/overview).\n",
    "\n",
    "This notebook is structured as follows:\n",
    "\n",
    "1. Data Sources\n",
    "    - Datasets used (including sources and descriptions)\n",
    "    - Preprocessing, splitting and uploading to BigQuery\n",
    "\n",
    "2. (Approach 1) Recipe Profiles Generation\n",
    "    - Profile schema design & prompt engineering\n",
    "    - Using BigQuery AI to generate recipe profiles\n",
    "\n",
    "3. (Approach 1) User Profiles Generation\n",
    "    - Profile schema design & prompt engineering\n",
    "    - Using BigQuery AI to generate user profiles\n",
    "\n",
    "4. (Approach 2) Vector Search vs ALS\n",
    "    - Using BigQuery AI to generate text embeddings\n",
    "    - Simple collaborative filtering\n",
    "    - Vector search for recommendations\n",
    "        - Naive approach\n",
    "        - HyDE (Hypothetical Document Embedding) approach\n",
    "    - Comparing results with ALS baseline\n",
    "\n",
    "5. (Approach 1) Explanation Generation\n",
    "    - Prompt engineering for explanation generation\n",
    "    - Using BigQuery AI to generate explanations for recommendations\n",
    "\n",
    "6. LLM-as-a-Judge as middle ground between Offline Metrics and A/B Testing\n",
    "\n",
    "7. (Approach 2) UI with Streamlit \n",
    "    - Hands-on interface to browse recommendations and their rationales\n",
    "        - Automatic Personalized Recommendations\n",
    "        - Semantic Search of Recipes\n",
    "\n",
    "8. Summary and Conclusions\n",
    "    - Feedback on BigQuery AI features\n",
    "    - User Survey on BigQuery AI features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595f34d",
   "metadata": {},
   "source": [
    "## Setup & Project Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86dc428f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os, json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "import os, ast, numpy as np, pandas as pd\n",
    "import random\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "import bigframes.pandas as bpd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825cb3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=O9sgSLDg97KGQC0ItenRtLdXnk7idd&access_type=offline&code_challenge=KwwEZ3fHzDUHOM-TptokMmbrmQ6LLmMzBxrJWQK1Hd8&code_challenge_method=S256\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "You are now logged in as [jeremy.matos@utec.edu.pe].\n",
      "Your current project is [kaggle-bigquery-471522].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n",
      "Updated property [core/project].\n",
      "\n",
      "Credentials saved to file: [/home/tenken/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"kaggle-bigquery-471522\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "os.environ['PROJECT_ID'] = input(\"Enter your Google Cloud Project ID: \")\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "\n",
    "subprocess.run(['gcloud', 'auth', 'login'])\n",
    "subprocess.run(['gcloud', 'config', 'set', 'project', PROJECT_ID])\n",
    "subprocess.run(['gcloud', 'auth', 'application-default', 'set-quota-project', PROJECT_ID])\n",
    "\n",
    "bpd.options.bigquery.project = PROJECT_ID\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "CONNECTION_ID = 'us.kaggle-connection'\n",
    "SCHEMA_NAME = 'deliverable'\n",
    "\n",
    "VALID_INTERACTIONS = f\"{PROJECT_ID}.{SCHEMA_NAME}.valid_interactions_windowed\"\n",
    "TRAIN_INTERACTIONS = f\"{PROJECT_ID}.{SCHEMA_NAME}.train_interactions_windowed\"\n",
    "SUBSET_RECIPE_IDS = f\"{PROJECT_ID}.{SCHEMA_NAME}.final_recipes\"\n",
    "SUBSET_USERS_IDS = f\"{PROJECT_ID}.{SCHEMA_NAME}.final_users\"\n",
    "\n",
    "RECIPES_ALL = f\"{PROJECT_ID}.{SCHEMA_NAME}.recipes\"\n",
    "OUT_DIM = 1024\n",
    "\n",
    "RECIPES_PARSED = f'{SCHEMA_NAME}.recipes_parsed'\n",
    "RECIPES_PROFILES_TABLE = f\"{SCHEMA_NAME}.recipe_profiles\"\n",
    "\n",
    "USERS_PARSED = f'{SCHEMA_NAME}.users_parsed'\n",
    "USERS_PROFILES_TABLE = f\"{SCHEMA_NAME}.user_profiles\"\n",
    "\n",
    "VECTOR_SEARCH_RESULTS_TABLE = f\"{SCHEMA_NAME}.vector_search_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88f30a",
   "metadata": {},
   "source": [
    "We use a [BigQuery client](https://cloud.google.com/bigquery/docs/datasets) to interact with the service in a more Pythonic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd66e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a26e4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset kaggle-bigquery-471522.deliverable\n"
     ]
    }
   ],
   "source": [
    "dataset = bigquery.Dataset(f\"{client.project}.{SCHEMA_NAME}\")\n",
    "dataset.location = \"US\"\n",
    "\n",
    "dataset = client.create_dataset(dataset, timeout=30)\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb70b5",
   "metadata": {},
   "source": [
    "We create a Cloud Resource connection to interact with Vertex AI services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9763acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Already Exists: Connection\n",
      "projects/352240171839/locations/us/connections/kaggle-connection\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bq', 'mk', '--connection', '--location=us', '--connection_type=CLOUD_RESOURCE', 'kaggle-connection'], returncode=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['bq', 'mk', '--connection', '--location=us', '--connection_type=CLOUD_RESOURCE', f'{CONNECTION_ID.replace(\"us.\", \"\")}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b9457",
   "metadata": {},
   "source": [
    "Following the tutorials on [GitHub](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb) we create a remote connection to the Text Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86acbae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7ddd8cd45a80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.query_and_wait(f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{SCHEMA_NAME}.text_embedding_model`\n",
    "REMOTE WITH \n",
    "    CONNECTION `{CONNECTION_ID}`\n",
    "    OPTIONS (ENDPOINT = 'gemini-embedding-001');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28699f8",
   "metadata": {},
   "source": [
    "# Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179a96d",
   "metadata": {},
   "source": [
    "For the dataset selection, three considerations were made:\n",
    "- The datasets are publicly available (in line with the competition guidelines)\n",
    "- The dataset should align with the goal of addressing a real-world problem using solutions that integrate naturally with SQL workflows.\n",
    "- The dataset needs to include unstructured, messy, and mixed data\n",
    "\n",
    "\n",
    "We utilize the [FoodRecSysV1](https://www.kaggle.com/datasets/elisaxxygao/foodrecsysv1) dataset from Kaggle, which provides comprehensive user interactions with recipes; including ratings, comments, and rich recipe metadata. This dataset closely mirrors the real-world challenges encountered by platforms such as Cookpad, DoorDash, UberEats, and Rappi.\n",
    "\n",
    "Other datasets like [MealRec](https://arxiv.org/abs/2205.12133) and [MealRec++](https://github.com/WUT-IDEA/MealRecPlus) were also considered (and we can apply the same methodology of this notebook), but for the sake of simplicity and ease to start, we opted for FoodRecSysV1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "105e2ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/elisaxxygao/foodrecsysv1?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.27G/5.27G [02:58<00:00, 31.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/tenken/.cache/kagglehub/datasets/elisaxxygao/foodrecsysv1/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"elisaxxygao/foodrecsysv1\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4b8df08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw-data_interaction.csv',\n",
       " 'core-data-valid_rating.csv',\n",
       " 'core-data_recipe.csv',\n",
       " 'raw-data-images',\n",
       " 'core-data-test_rating.csv',\n",
       " 'core-data-train_rating.csv',\n",
       " 'core-data-images',\n",
       " 'raw-data_recipe.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6529d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "user_id                     object\n",
       "recipe_id                   object\n",
       "rating                     float64\n",
       "dateLastModified    datetime64[ns]\n",
       "month                        int32\n",
       "quarter                      int32\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>dateLastModified</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>676946.000</td>\n",
       "      <td>676946</td>\n",
       "      <td>676946.000</td>\n",
       "      <td>676946.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.469</td>\n",
       "      <td>2008-10-12 19:03:51.871918336</td>\n",
       "      <td>6.496</td>\n",
       "      <td>2.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>2000-02-08 12:09:11.987000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000</td>\n",
       "      <td>2007-07-20 09:30:30.988250112</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000</td>\n",
       "      <td>2009-05-15 03:51:38.998499840</td>\n",
       "      <td>7.000</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000</td>\n",
       "      <td>2010-08-31 19:02:24.820499968</td>\n",
       "      <td>10.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000</td>\n",
       "      <td>2011-10-14 17:43:08.433000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.574</td>\n",
       "      <td>1.148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rating               dateLastModified      month    quarter\n",
       "count 676946.000                         676946 676946.000 676946.000\n",
       "mean       4.469  2008-10-12 19:03:51.871918336      6.496      2.502\n",
       "min        1.000     2000-02-08 12:09:11.987000      1.000      1.000\n",
       "25%        4.000  2007-07-20 09:30:30.988250112      3.000      1.000\n",
       "50%        5.000  2009-05-15 03:51:38.998499840      7.000      3.000\n",
       "75%        5.000  2010-08-31 19:02:24.820499968     10.000      4.000\n",
       "max        5.000     2011-10-14 17:43:08.433000     12.000      4.000\n",
       "std        0.860                            NaN      3.574      1.148"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core_train_rating.shape=(676946, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "user_id                     object\n",
       "recipe_id                   object\n",
       "rating                     float64\n",
       "dateLastModified    datetime64[ns]\n",
       "month                        int32\n",
       "quarter                      int32\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>dateLastModified</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>283440.000</td>\n",
       "      <td>283440</td>\n",
       "      <td>283440.000</td>\n",
       "      <td>283440.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.510</td>\n",
       "      <td>2015-02-09 01:26:51.432288512</td>\n",
       "      <td>6.310</td>\n",
       "      <td>2.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>2013-01-31 22:55:07.660000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000</td>\n",
       "      <td>2013-11-26 10:55:44.029000192</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000</td>\n",
       "      <td>2014-11-15 07:59:59.511500032</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000</td>\n",
       "      <td>2016-03-13 10:05:13.634749952</td>\n",
       "      <td>10.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000</td>\n",
       "      <td>2018-03-15 03:09:12.853000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.595</td>\n",
       "      <td>1.161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rating               dateLastModified      month    quarter\n",
       "count 283440.000                         283440 283440.000 283440.000\n",
       "mean       4.510  2015-02-09 01:26:51.432288512      6.310      2.438\n",
       "min        1.000     2013-01-31 22:55:07.660000      1.000      1.000\n",
       "25%        4.000  2013-11-26 10:55:44.029000192      3.000      1.000\n",
       "50%        5.000  2014-11-15 07:59:59.511500032      6.000      2.000\n",
       "75%        5.000  2016-03-13 10:05:13.634749952     10.000      4.000\n",
       "max        5.000     2018-03-15 03:09:12.853000     12.000      4.000\n",
       "std        0.851                            NaN      3.595      1.161"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'val'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "user_id                     object\n",
       "recipe_id                   object\n",
       "rating                     float64\n",
       "dateLastModified    datetime64[ns]\n",
       "month                        int32\n",
       "quarter                      int32\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>dateLastModified</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>133459.000</td>\n",
       "      <td>133459</td>\n",
       "      <td>133459.000</td>\n",
       "      <td>133459.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.506</td>\n",
       "      <td>2012-05-23 09:30:25.575911936</td>\n",
       "      <td>6.870</td>\n",
       "      <td>2.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>2011-10-14 17:46:56.443000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000</td>\n",
       "      <td>2012-01-16 12:26:58.273499904</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000</td>\n",
       "      <td>2012-05-11 17:51:50.223000064</td>\n",
       "      <td>7.000</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000</td>\n",
       "      <td>2012-09-24 14:10:02.375000064</td>\n",
       "      <td>11.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000</td>\n",
       "      <td>2013-01-31 22:23:27.613000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.978</td>\n",
       "      <td>1.245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rating               dateLastModified      month    quarter\n",
       "count 133459.000                         133459 133459.000 133459.000\n",
       "mean       4.506  2012-05-23 09:30:25.575911936      6.870      2.643\n",
       "min        1.000     2011-10-14 17:46:56.443000      1.000      1.000\n",
       "25%        4.000  2012-01-16 12:26:58.273499904      3.000      1.000\n",
       "50%        5.000  2012-05-11 17:51:50.223000064      7.000      3.000\n",
       "75%        5.000  2012-09-24 14:10:02.375000064     11.000      4.000\n",
       "max        5.000     2013-01-31 22:23:27.613000     12.000      4.000\n",
       "std        0.830                            NaN      3.978      1.245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_df(core: pd.DataFrame) -> pd.DataFrame:\n",
    "    core['user_id'] = core['user_id'].astype(str)\n",
    "    core['recipe_id'] = core['recipe_id'].astype(str)\n",
    "    core['dateLastModified'] = core['dateLastModified'].apply(lambda v: v.replace('\\n', ''))\n",
    "    core['dateLastModified'] = pd.to_datetime(core['dateLastModified'], format='ISO8601')\n",
    "    core['month'] = core['dateLastModified'].dt.month\n",
    "    core['quarter'] = core['dateLastModified'].dt.quarter\n",
    "    core['rating'] = core['rating'].astype(float)\n",
    "    \n",
    "    core = core.dropna(how='any').sort_values(by=['dateLastModified'], ascending=[True])\n",
    "    return core\n",
    "\n",
    "core_train_rating = pd.read_csv(f'{path}/core-data-train_rating.csv')\n",
    "core_train_rating = clean_df(core_train_rating)\n",
    "display('train', core_train_rating.dtypes, core_train_rating.describe())\n",
    "print(f\"{core_train_rating.shape=}\")\n",
    "\n",
    "\n",
    "core_test_rating = pd.read_csv(f'{path}/core-data-test_rating.csv')\n",
    "core_test_rating = clean_df(core_test_rating)\n",
    "display('test', core_test_rating.dtypes, core_test_rating.describe())\n",
    "\n",
    "\n",
    "core_val_rating = pd.read_csv(f'{path}/core-data-valid_rating.csv')\n",
    "core_val_rating = clean_df(core_val_rating)\n",
    "display('val', core_val_rating.dtypes, core_val_rating.describe())\n",
    "\n",
    "recipes = pd.read_csv('./data/food_recsys/raw-data_recipe.csv')\n",
    "recipes['recipe_id'] = recipes['recipe_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c72f4a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using last 24 weeks of training data to predict next 4 weeks of ratings, with at least 5 interactions in training and 5 interactions in validation for each user\n",
      "From 2011-04-29 17:43:08.433000 to 2011-11-11 17:46:56.443000\n"
     ]
    }
   ],
   "source": [
    "PREV_WEEKS = 24\n",
    "POST_WEEKS = 4\n",
    "MIN_INTERACTIONS_TRAIN = 5\n",
    "MIN_INTERACTIONS_VAL = 5\n",
    "print(f\"Using last {PREV_WEEKS} weeks of training data to predict next {POST_WEEKS} weeks of ratings, with at least {MIN_INTERACTIONS_TRAIN} interactions in training and {MIN_INTERACTIONS_VAL} interactions in validation for each user\")\n",
    "\n",
    "min_date_val = core_train_rating['dateLastModified'].max() - pd.Timedelta(weeks=PREV_WEEKS)\n",
    "max_date_val = core_val_rating['dateLastModified'].min() + pd.Timedelta(weeks=POST_WEEKS)\n",
    "\n",
    "print(f\"From {min_date_val} to {max_date_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d30af3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(common_users)=3729, len(common_recipes)=3764\n",
      "len(common_users)=2942, len(common_recipes)=2628\n",
      "Final datasets: 2730 train interactions, 1066 val interactions\n",
      "Min interactions per user in train: 5\n",
      "Min interactions per user in val: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'train_users'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "1037986    176\n",
       "2043209    105\n",
       "2448319    104\n",
       "2702518     80\n",
       "2995814     70\n",
       "          ... \n",
       "2233245      5\n",
       "6392312      5\n",
       "3500863      5\n",
       "2524829      5\n",
       "4053273      5\n",
       "Name: count, Length: 131, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'val_users'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "1037986    31\n",
       "2995814    29\n",
       "2043209    29\n",
       "2448319    24\n",
       "6067445    21\n",
       "           ..\n",
       "3047421     5\n",
       "484578      5\n",
       "5017867     5\n",
       "301943      5\n",
       "1054570     5\n",
       "Name: count, Length: 131, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 131, Recipes: 2048\n"
     ]
    }
   ],
   "source": [
    "# Use training data of last N weeks only\n",
    "core_train_rating = core_train_rating.loc[\n",
    "    lambda df: df['dateLastModified'] >= min_date_val\n",
    "]\n",
    "\n",
    "# Reduce val predictions to next POST_WEEKS only\n",
    "core_val_rating = core_val_rating.loc[\n",
    "    lambda df: (df['dateLastModified'] <= max_date_val) & (df['rating'] >= 3)\n",
    "]\n",
    "\n",
    "# Find common users and recipes first\n",
    "common_users = set(core_train_rating['user_id']).intersection(set(core_val_rating['user_id']))\n",
    "common_recipes = set(core_train_rating['recipe_id']).intersection(set(core_val_rating['recipe_id']))\n",
    "print(f\"{len(common_users)=}, {len(common_recipes)=}\")\n",
    "\n",
    "# Filter both datasets to only include common users and recipes\n",
    "train_users = core_train_rating[\n",
    "    core_train_rating['user_id'].isin(common_users) & \n",
    "    core_train_rating['recipe_id'].isin(common_recipes)\n",
    "]\n",
    "val_users = core_val_rating[\n",
    "    core_val_rating['user_id'].isin(common_users) & \n",
    "    core_val_rating['recipe_id'].isin(common_recipes)\n",
    "]\n",
    "\n",
    "common_users = set(train_users['user_id']).intersection(set(val_users['user_id']))\n",
    "common_recipes = set(train_users['recipe_id']).intersection(set(val_users['recipe_id']))\n",
    "print(f\"{len(common_users)=}, {len(common_recipes)=}\")\n",
    "\n",
    "# Now filter by minimum interactions AFTER filtering by common users/recipes\n",
    "train_user_counts = train_users['user_id'].value_counts()\n",
    "val_user_counts = val_users['user_id'].value_counts()\n",
    "train_recipe_counts = train_users['recipe_id'].value_counts()\n",
    "val_recipe_counts = val_users['recipe_id'].value_counts()\n",
    "\n",
    "# Users and recipes with at least MIN_INTERACTIONS interactions in FINAL filtered datasets\n",
    "users_min_it_train = set(train_user_counts[train_user_counts >= MIN_INTERACTIONS_TRAIN].index)\n",
    "users_min_it_val = set(val_user_counts[val_user_counts >= MIN_INTERACTIONS_VAL].index)\n",
    "\n",
    "# Final common users and recipes with minimum interactions\n",
    "final_users = users_min_it_train.intersection(users_min_it_val)\n",
    "\n",
    "# Apply final filter\n",
    "train_users = train_users[train_users['user_id'].isin(final_users)]\n",
    "val_users = val_users[(val_users['user_id'].isin(final_users))]\n",
    "final_recipes = set(train_users['recipe_id'].values).union(set(val_users['recipe_id'].values))\n",
    "\n",
    "# Use final recipes for all recipe datasets\n",
    "train_recipes = val_recipes = recipes[recipes['recipe_id'].isin(final_recipes)]\n",
    "\n",
    "\n",
    "print(f\"Final datasets: {len(train_users)} train interactions, {len(val_users)} val interactions\")\n",
    "\n",
    "# Verify minimum interactions constraint\n",
    "print(f\"Min interactions per user in train: {train_users['user_id'].value_counts().min()}\")\n",
    "print(f\"Min interactions per user in val: {val_users['user_id'].value_counts().min()}\")\n",
    "\n",
    "display('train_users', train_users['user_id'].value_counts().sort_values(ascending=False))\n",
    "display('val_users', val_users['user_id'].value_counts().sort_values(ascending=False))\n",
    "print(f\"Users: {len(final_users)}, Recipes: {len(final_recipes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59dd96f",
   "metadata": {},
   "source": [
    "We'll finally use a total of 131 users and 2048 recipes, a size that balances data volume and execution time for demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "beb05c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9965/349922751.py:3: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  train_users.to_gbq(TRAIN_INTERACTIONS, if_exists='replace')\n",
      "100%|██████████| 1/1 [00:00<00:00, 7476.48it/s]\n",
      "/tmp/ipykernel_9965/349922751.py:4: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  val_users.to_gbq(VALID_INTERACTIONS, if_exists='replace')\n",
      "100%|██████████| 1/1 [00:00<00:00, 10155.70it/s]\n",
      "/tmp/ipykernel_9965/349922751.py:5: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  train_recipes[['recipe_id']].to_gbq(SUBSET_RECIPE_IDS, if_exists='replace')\n",
      "100%|██████████| 1/1 [00:00<00:00, 8338.58it/s]\n",
      "/tmp/ipykernel_9965/349922751.py:6: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  pd.DataFrame({'user_id': list(final_users)}).to_gbq(SUBSET_USERS_IDS, if_exists='replace')\n",
      "100%|██████████| 1/1 [00:00<00:00, 6594.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Upload tables to BigQuery\n",
    "\n",
    "train_users.to_gbq(TRAIN_INTERACTIONS, if_exists='replace')\n",
    "val_users.to_gbq(VALID_INTERACTIONS, if_exists='replace')\n",
    "train_recipes[['recipe_id']].to_gbq(SUBSET_RECIPE_IDS, if_exists='replace')\n",
    "pd.DataFrame({'user_id': list(final_users)}).to_gbq(SUBSET_USERS_IDS, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1bf359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9965/1013056983.py:1: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  recipes.to_gbq(RECIPES_ALL, if_exists='replace')\n",
      "100%|██████████| 1/1 [00:00<00:00, 8081.51it/s]\n"
     ]
    }
   ],
   "source": [
    "recipes.to_gbq(RECIPES_ALL, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c4f8c",
   "metadata": {},
   "source": [
    "Next, we're going to generate profiles in line with the [Spotify blog](https://research.atspotify.com/2025/9/profile-aware-llm-as-a-judge-for-podcasts-a-better-middle-ground-between). But first, let's define some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19c94fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_to_prompt_with_descriptions(model_class) -> str:\n",
    "    prompt = \"\"\n",
    "    for k, v in model_class.model_json_schema()['properties'].items():\n",
    "        desc = v.get('description', '')\n",
    "        prompt += f\" {k} ({desc}) \"\n",
    "    return f\"[ {prompt} ]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24b5bb",
   "metadata": {},
   "source": [
    "# Recipe Profiles Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf54445",
   "metadata": {},
   "source": [
    "We start cleaning and parsing some columns to make them suitable and more understandable for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abb42144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ingredients(text: str) -> str:\n",
    "    if pd.isna(text): return \"\"\n",
    "    return \"\\n\".join([f\"- {v}\" for v in str(text).split('^')])\n",
    "\n",
    "\n",
    "def prep_directions(text: str) -> str:\n",
    "    if pd.isna(text): return \"\"\n",
    "    s = str(text)\n",
    "    # Some rows look like dict-strings with 'directions' inside; just fall back to raw text\n",
    "    # Optionally, try to parse if it starts with \"{\"\n",
    "    if s.strip().startswith(\"{\"):\n",
    "        try:\n",
    "            d = ast.literal_eval(s)\n",
    "            # common keys: 'directions' (string) or list\n",
    "            v = d.get('directions', \"\")\n",
    "            v = str(v).split('\\n')\n",
    "            v = [x.strip() for x in v if len(x.strip()) > 0]\n",
    "            v = [f\". {x}\" if x and x[0].isupper() else x for x in v]\n",
    "\n",
    "            return \" \".join(v).strip(\".\").replace(\" . \", \". \").replace(\"..\", \".\").strip()\n",
    "        except Exception:\n",
    "            return s.lower()\n",
    "    return s.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5c1bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipes = bpd.read_gbq(f\"\"\"\n",
    "SELECT * FROM `{SUBSET_RECIPE_IDS}`\n",
    "LEFT JOIN `{RECIPES_ALL}` USING(recipe_id)\n",
    "\"\"\")\n",
    "\n",
    "# Convert to pandas DataFrame to use custom functions, then back to BigFrames\n",
    "df_recipes_pandas = df_recipes.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c194da04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2048/2048 [00:00<00:00, 2261.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Load job 82f7ea86-d8b3-4b3f-81ba-724478507801 is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=kaggle-bigquery-471522&j=bq:US:82f7ea86-d8b3-4b3f-81ba-724478507801&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 9655fc08-f46d-43b2-85fd-458eafe3d8f3 is DONE. 571.7 MB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=kaggle-bigquery-471522&j=bq:US:9655fc08-f46d-43b2-85fd-458eafe3d8f3&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'kaggle-bigquery-471522.deliverable.recipes_parsed'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutrition_values = []\n",
    "for idx, row in tqdm(df_recipes_pandas.iterrows(), total=len(df_recipes_pandas)):\n",
    "    nutritions_dict = ast.literal_eval(row['nutritions'])\n",
    "    \n",
    "    row_info = {'recipe_id': row['recipe_id']}\n",
    "    nutritions_info = {}\n",
    "    for k in ['niacin', 'sugars', 'sodium', 'carbohydrates', 'vitaminB6', 'calories', 'thiamin', 'fat', 'folate', 'caloriesFromFat', 'calcium', 'fiber', 'magnesium', 'iron', 'cholesterol', 'protein', 'vitaminA', 'potassium', 'saturatedFat', 'vitaminC']:\n",
    "        if k in nutritions_dict:\n",
    "            nutritions_info[k] = nutritions_dict[k].get('percentDailyValue', -1)\n",
    "            if nutritions_info[k] is not None:\n",
    "                v = str(nutritions_info[k]).strip()\n",
    "                if v == '< 1':\n",
    "                    nutritions_info[k] = 0.0\n",
    "                # if v == '-':\n",
    "                #     nutritions_info[k] = -1\n",
    "                \n",
    "                try:\n",
    "                    nutritions_info[k] = f\"{nutritions_info[k]} percent\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "    row_info['percent_daily_values'] = \"\\n\".join([f\"{k}: {v}\" for k, v in nutritions_info.items()])\n",
    "    nutrition_values.append(row_info)\n",
    "\n",
    "nutrition_df = pd.DataFrame(nutrition_values).fillna(-2)\n",
    "\n",
    "df_recipes_pandas['parsed_ingredients'] = df_recipes_pandas['ingredients'].apply(prep_ingredients)\n",
    "df_recipes_pandas['parsed_recipe'] = df_recipes_pandas['cooking_directions'].apply(prep_directions)\n",
    "df_recipes_pandas = df_recipes_pandas.merge(nutrition_df, how='left', on='recipe_id')\n",
    "df_recipes = bpd.DataFrame(df_recipes_pandas)\n",
    "\n",
    "# Upload the new table in BigQuery\n",
    "df_recipes.to_gbq(\n",
    "    destination_table=f\"{PROJECT_ID}.{RECIPES_PARSED}\",\n",
    "    if_exists='replace',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e35ffb",
   "metadata": {},
   "source": [
    "Following the [DoorDash blog](https://careersatdoordash.com/blog/doordash-profile-generation-llms-understanding-consumers-merchants-and-items/) we define the structure and attributes of the \"Recipe Profile\" (item profiles). In this case we use a Pydantic model to define the schema, which then parsed as part of the prompt using the function `schema_to_prompt_with_descriptions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70b85692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeProfile(BaseModel):\n",
    "    food_type: str = Field(description=\"Type of food, e.g., dessert, main course, appetizer\")\n",
    "    cuisine_type: str = Field(description=\"Cuisine type, e.g., Italian, Chinese, Mexican, American\")\n",
    "    dietary_preferences: List[str] = Field(description=\"Dietary preferences, e.g., omnivore, vegetarian, vegan, gluten-free\")\n",
    "    flavor_profile: List[str] = Field(description=\"Flavor profile, e.g., spicy, sweet, savory\")\n",
    "    serving_daypart: List[str] = Field(description=\"Suitable dayparts, e.g., breakfast, lunch, dinner\")\n",
    "    notes: str = Field(description=\"Short rationale summarizing the recipe profile\")\n",
    "    target_audience: str = Field(description=\"Types of users who would likely enjoy this recipe based on cooking skill level, flavor intensity, dietary needs, and lifestyle preferences. Helps recommendation systems match recipes to appropriate user profiles.\")\n",
    "    justification: str = Field(description=\"Detailed explanation of how the profile was determined Describe why the food type, cuisine type, dietary preferences, flavor profile, and serving daypart were chosen based on the ingredients and cooking directions. Is not allowed to use quotes or complex punctuation in this field.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fa17e",
   "metadata": {},
   "source": [
    "This schema could be enhanced by using more restrictive field types, such as `Enums` or `Literal` for categorical attributes. This would make it easier to apply business logic based on the extracted category metadata generated by the LLM in future iterations. \n",
    "\n",
    "Then, we define the prompt template to generate the recipe profiles, as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "744fd3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the title, ingredients, cooking directions and percent daily values provided, create a recipe profile that summarizes the key characteristics of this recipe. Your response must follow this exact structure: [  food_type (Type of food, e.g., dessert, main course, appetizer)  cuisine_type (Cuisine type, e.g., Italian, Chinese, Mexican, American)  dietary_preferences (Dietary preferences, e.g., omnivore, vegetarian, vegan, gluten-free)  flavor_profile (Flavor profile, e.g., spicy, sweet, savory)  serving_daypart (Suitable dayparts, e.g., breakfast, lunch, dinner)  notes (Short rationale summarizing the recipe profile)  target_audience (Types of users who would likely enjoy this recipe based on cooking skill level, flavor intensity, dietary needs, and lifestyle preferences. Helps recommendation systems match recipes to appropriate user profiles.)  justification (Detailed explanation of how the profile was determined Describe why the food type, cuisine type, dietary preferences, flavor profile, and serving daypart were chosen based on the ingredients and cooking directions. Is not allowed to use quotes or complex punctuation in this field.)  ]. IMPORTANT: Do not use quotation marks or complex punctuation in your response. Use simple words and avoid any quotes, apostrophes, or special characters.\n"
     ]
    }
   ],
   "source": [
    "recipe_profile_prompt = f\"\"\"Based on the title, ingredients, cooking directions and percent daily values provided, create a recipe profile that summarizes the key characteristics of this recipe. Your response must follow this exact structure: {schema_to_prompt_with_descriptions(RecipeProfile)}. IMPORTANT: Do not use quotation marks or complex punctuation in your response. Use simple words and avoid any quotes, apostrophes, or special characters.\"\"\"\n",
    "\n",
    "print(recipe_profile_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb9f7763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH ai_responses AS (\n",
      "  SELECT \n",
      "    s.recipe_id, \n",
      "    s.title, \n",
      "    s.ingredients, \n",
      "    s.cooking_directions, \n",
      "    s.nutritions, \n",
      "    s.reviews, \n",
      "    s.parsed_ingredients, \n",
      "    s.parsed_recipe,\n",
      "    AI.GENERATE(('Based on the title, ingredients, cooking directions and percent daily values provided, create a recipe profile that summarizes the key characteristics of this recipe. Your response must follow this exact structure: [  food_type (Type of food, e.g., dessert, main course, appetizer)  cuisine_type (Cuisine type, e.g., Italian, Chinese, Mexican, American)  dietary_preferences (Dietary preferences, e.g., omnivore, vegetarian, vegan, gluten-free)  flavor_profile (Flavor profile, e.g., spicy, sweet, savory)  serving_daypart (Suitable dayparts, e.g., breakfast, lunch, dinner)  notes (Short rationale summarizing the recipe profile)  target_audience (Types of users who would likely enjoy this recipe based on cooking skill level, flavor intensity, dietary needs, and lifestyle preferences. Helps recommendation systems match recipes to appropriate user profiles.)  justification (Detailed explanation of how the profile was determined Describe why the food type, cuisine type, dietary preferences, flavor profile, and serving daypart were chosen based on the ingredients and cooking directions. Is not allowed to use quotes or complex punctuation in this field.)  ]. IMPORTANT: Do not use quotation marks or complex punctuation in your response. Use simple words and avoid any quotes, apostrophes, or special characters.', s.parsed_ingredients, s.parsed_recipe, s.percent_daily_values),\n",
      "        connection_id => 'us.kaggle-connection',\n",
      "        endpoint => 'gemini-2.5-flash',\n",
      "        model_params => JSON '{\"generationConfig\":{\"temperature\": 1.0, \"maxOutputTokens\": 2048, \"thinking_config\": {\"thinking_budget\": 1024} } }',\n",
      "        output_schema => 'food_type STRING, cuisine_type STRING, dietary_preferences ARRAY<STRING>, flavor_profile ARRAY<STRING>, serving_daypart ARRAY<STRING>, notes STRING, target_audience STRING, justification STRING'\n",
      "    ) AS ai_result\n",
      "  FROM (SELECT * FROM `deliverable.recipes_parsed`) s\n",
      ")\n",
      "SELECT \n",
      "  *,\n",
      "  ai_result.full_response AS recipe_profile,\n",
      "  JSON_EXTRACT_SCALAR(ai_result.full_response, '$.candidates[0].content.parts[0].text') AS recipe_profile_text\n",
      "FROM ai_responses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recipe_profile_generation_query = f\"\"\"\n",
    "WITH ai_responses AS (\n",
    "  SELECT \n",
    "    s.recipe_id, \n",
    "    s.title, \n",
    "    s.ingredients, \n",
    "    s.cooking_directions, \n",
    "    s.nutritions, \n",
    "    s.reviews, \n",
    "    s.parsed_ingredients, \n",
    "    s.parsed_recipe,\n",
    "    AI.GENERATE(('{recipe_profile_prompt}', s.parsed_ingredients, s.parsed_recipe, s.percent_daily_values),\n",
    "        connection_id => '{CONNECTION_ID}',\n",
    "        endpoint => 'gemini-2.5-flash',\n",
    "        model_params => JSON '{{\"generationConfig\":{{\"temperature\": 1.0, \"maxOutputTokens\": 2048, \"thinking_config\": {{\"thinking_budget\": 1024}} }} }}',\n",
    "        output_schema => 'food_type STRING, cuisine_type STRING, dietary_preferences ARRAY<STRING>, flavor_profile ARRAY<STRING>, serving_daypart ARRAY<STRING>, notes STRING, target_audience STRING, justification STRING'\n",
    "    ) AS ai_result\n",
    "  FROM (SELECT * FROM `{RECIPES_PARSED}`) s\n",
    ")\n",
    "SELECT \n",
    "  *,\n",
    "  ai_result.full_response AS recipe_profile,\n",
    "  JSON_EXTRACT_SCALAR(ai_result.full_response, '$.candidates[0].content.parts[0].text') AS recipe_profile_text\n",
    "FROM ai_responses\n",
    "\"\"\"\n",
    "\n",
    "print(recipe_profile_generation_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run profile generation query and save results to RECIPES_PROFILES_TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095acf73",
   "metadata": {},
   "source": [
    "# User Profiles Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a978888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipe_metadata = client.query_and_wait(f\"\"\"SELECT recipe_id, title, parsed_ingredients, parsed_recipe, recipe_profile_text, reviews FROM `{RECIPES_PROFILES_TABLE}`\"\"\").to_dataframe()\n",
    "\n",
    "reviews = []\n",
    "for idx, row in tqdm(df_recipe_metadata.iterrows(), total=len(df_recipe_metadata)):\n",
    "    recipe_id = row['recipe_id']\n",
    "    interactions_dict = ast.literal_eval(row['reviews'])\n",
    "    for k, v in interactions_dict.items():\n",
    "        reviews.append({\n",
    "            'recipe_id': recipe_id,\n",
    "            'user_id': str(k),\n",
    "            **v\n",
    "        })\n",
    "reviews_df = pd.DataFrame(reviews)\n",
    "reviews_df.columns = reviews_df.columns.str.lower()\n",
    "reviews_df['datelastmodified'] = pd.to_datetime(reviews_df['datelastmodified'], format='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_COLS = 'user_id, recipe_id, rating, datelastmodified'\n",
    "df_train_users = client.query_and_wait(f\"\"\"SELECT {SUBSET_COLS} FROM `{TRAIN_INTERACTIONS}`\"\"\").to_dataframe()\n",
    "df_valid_users = client.query_and_wait(f\"\"\"SELECT {SUBSET_COLS} FROM `{VALID_INTERACTIONS}`\"\"\").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ada9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop users in valid not present in train_set\n",
    "FINAL_USERS = set(df_train_users['user_id'].unique()).intersection(set(df_valid_users['user_id'].unique()))\n",
    "print(f\"Final users: {len(FINAL_USERS)}\")\n",
    "\n",
    "df_users_history = df_train_users[df_train_users['user_id'].isin(FINAL_USERS)].reset_index(drop=True)\n",
    "df_users_history['datelastmodified'] = pd.to_datetime(df_users_history['datelastmodified'])\n",
    "df_users_history = df_users_history.merge(\n",
    "    reviews_df[['user_id', 'recipe_id', 'datelastmodified', 'text']], how='left', \n",
    "    on=['user_id', 'recipe_id', 'datelastmodified'],\n",
    "    validate='one_to_one'\n",
    ").rename(columns={'text': 'user_comment'})\n",
    "\n",
    "df_valid_users = df_valid_users[df_valid_users['user_id'].isin(FINAL_USERS)].reset_index(drop=True)\n",
    "\n",
    "print(df_users_history.describe())\n",
    "print(df_valid_users.describe())\n",
    "\n",
    "df_users_to_profile = df_valid_users.groupby('user_id').agg({'recipe_id': 'unique'}).reset_index().rename(columns={\n",
    "    'recipe_id': 'rec_gt'\n",
    "})  # , 'datelastmodified'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ec940",
   "metadata": {},
   "source": [
    "For the user profile generation, we take advantage of the context window size of the Gemini models to provide a list of their historical interactions (ratings and comments) as part of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_history(user_id: int, n: int = 25, k_min: int = 5) -> list:\n",
    "    pass\n",
    "\n",
    "def format_user_history(user_history: list[dict]) -> str:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51563b",
   "metadata": {},
   "source": [
    "As in the recipe profiles, we define a Pydantic model, that it can be improved and adapted to the specific business case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c64122a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserProfile(BaseModel):\n",
    "    liked_cuisines: List[str] = Field(description=\"List of cuisines the user enjoys most, ranked by preference based on their interaction history and ratings\")\n",
    "    cuisine_preference: str = Field(description=\"Primary cuisine type the user gravitates towards (e.g., Mediterranean, Asian Fusion, Traditional American)\")\n",
    "    dietary_preference: str = Field(description=\"Main dietary restriction or lifestyle the user follows (e.g., Vegetarian, Low-carb, No restrictions)\")\n",
    "\n",
    "    food_preferences: List[str] = Field(description=\"Preferred food categories and meal types (e.g., comfort food, healthy salads, baked goods, grilled meats)\")\n",
    "    top_cuisine_choices: List[str] = Field(description=\"Specific regional or ethnic cuisines the user frequently rates highly (e.g., Thai, Southern BBQ, French pastry)\")\n",
    "    dietary_preferences: List[str] = Field(description=\"Dietary restrictions, health considerations, or eating patterns (e.g., gluten-free, plant-based, high-protein, dairy-free)\")\n",
    "    flavor_preferences: List[str] = Field(description=\"Dominant taste profiles and flavor characteristics the user seeks (e.g., bold and spicy, mild and creamy, tangy and citrusy)\")\n",
    "    daypart_preferences: List[str] = Field(description=\"Preferred times of day for different meal types based on rating patterns (e.g., hearty breakfast, light lunch, elaborate dinner)\")\n",
    "    lifestyle_tags: List[str] = Field(description=\"Behavioral patterns and cooking style indicators inferred from recipe choices (e.g., quick meals, entertainer, health-conscious, experimental cook)\")\n",
    "    convenience_preference: str = Field(description=\"Preference for recipe complexity (e.g., quick and easy, gourmet elaborate)\")\n",
    "    diversity_openness: str = Field(description=\"Willingness to try new cuisines (e.g., adventurous, selective, traditionalist, not defined)\")\n",
    "\n",
    "    notes: str = Field(description=\"Brief summary explaining the users overall food personality and any notable patterns in their preferences\")\n",
    "    justification: str = Field(description=\"Detailed explanation of how the profile was determined based on the users interaction history and ratings. Describe why the liked cuisines, cuisine preference, dietary preference, food preferences, cuisine preferences, dietary preferences, flavor preferences, daypart preferences, and lifestyle tags were chosen. Is not allowed to use quotes or complex punctuation in this field. Keep it between 100 and 200 words not more.\")\n",
    "    user_story: str = Field(description=\"Predictive narrative about the user s culinary evolution and potential future preferences. Describes their food journey, emerging patterns, and likely directions for taste exploration. Written to help predict what they might enjoy next based on their current trajectory and evolving palate.\")\n",
    "    future_preferences: str = Field(description=\"Speculative insights into the types of recipes and cuisines the user may be inclined to explore in the future. Based on their current preferences, suggest new food categories, cooking styles, or dietary trends they might be open to trying next. This helps in anticipating their evolving culinary interests.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68e46f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a structured user profile that captures their culinary tastes, dietary preferences, flavor inclinations, among others. This user profile will be used then for a Recommendation System. Ensure the profile is concise, reasonable and accurately reflects the users food personality based on their interaction history. Please provide a structured profile of the user using the following format: [  liked_cuisines (List of cuisines the user enjoys most, ranked by preference based on their interaction history and ratings)  cuisine_preference (Primary cuisine type the user gravitates towards (e.g., Mediterranean, Asian Fusion, Traditional American))  dietary_preference (Main dietary restriction or lifestyle the user follows (e.g., Vegetarian, Low-carb, No restrictions))  food_preferences (Preferred food categories and meal types (e.g., comfort food, healthy salads, baked goods, grilled meats))  top_cuisine_choices (Specific regional or ethnic cuisines the user frequently rates highly (e.g., Thai, Southern BBQ, French pastry))  dietary_preferences (Dietary restrictions, health considerations, or eating patterns (e.g., gluten-free, plant-based, high-protein, dairy-free))  flavor_preferences (Dominant taste profiles and flavor characteristics the user seeks (e.g., bold and spicy, mild and creamy, tangy and citrusy))  daypart_preferences (Preferred times of day for different meal types based on rating patterns (e.g., hearty breakfast, light lunch, elaborate dinner))  lifestyle_tags (Behavioral patterns and cooking style indicators inferred from recipe choices (e.g., quick meals, entertainer, health-conscious, experimental cook))  convenience_preference (Preference for recipe complexity (e.g., quick and easy, gourmet elaborate))  diversity_openness (Willingness to try new cuisines (e.g., adventurous, selective, traditionalist, not defined))  notes (Brief summary explaining the users overall food personality and any notable patterns in their preferences)  justification (Detailed explanation of how the profile was determined based on the users interaction history and ratings. Describe why the liked cuisines, cuisine preference, dietary preference, food preferences, cuisine preferences, dietary preferences, flavor preferences, daypart preferences, and lifestyle tags were chosen. Is not allowed to use quotes or complex punctuation in this field. Keep it between 100 and 200 words not more.)  user_story (Predictive narrative about the user s culinary evolution and potential future preferences. Describes their food journey, emerging patterns, and likely directions for taste exploration. Written to help predict what they might enjoy next based on their current trajectory and evolving palate.)  future_preferences (Speculative insights into the types of recipes and cuisines the user may be inclined to explore in the future. Based on their current preferences, suggest new food categories, cooking styles, or dietary trends they might be open to trying next. This helps in anticipating their evolving culinary interests.)  ]. Each fill of the structured output doesnt need to take more than 200 words keep it in mind. IMPORTANT: Do not use quotation marks or complex punctuation in your response. Use simple words and avoid any quotes, apostrophes, or special characters. Use the following interaction history as reference:\n"
     ]
    }
   ],
   "source": [
    "user_profile_prompt = f\"\"\"Generate a structured user profile that captures their culinary tastes, dietary preferences, flavor inclinations, among others. This user profile will be used then for a Recommendation System. Ensure the profile is concise, reasonable and accurately reflects the users food personality based on their interaction history. Please provide a structured profile of the user using the following format: {schema_to_prompt_with_descriptions(UserProfile)}. Each fill of the structured output doesnt need to take more than 200 words keep it in mind. IMPORTANT: Do not use quotation marks or complex punctuation in your response. Use simple words and avoid any quotes, apostrophes, or special characters. Use the following interaction history as reference:\"\"\"\n",
    "\n",
    "print(user_profile_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b60f6005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH ai_responses AS (\n",
      "  SELECT \n",
      "    s.user_id, \n",
      "    s.n_history,\n",
      "    s.history_string,\n",
      "    AI.GENERATE(('Generate a structured user profile that captures their culinary tastes, dietary preferences, flavor inclinations, among others. This user profile will be used then for a Recommendation System. Ensure the profile is concise, reasonable and accurately reflects the users food personality based on their interaction history. Please provide a structured profile of the user using the following format: [  liked_cuisines (List of cuisines the user enjoys most, ranked by preference based on their interaction history and ratings)  cuisine_preference (Primary cuisine type the user gravitates towards (e.g., Mediterranean, Asian Fusion, Traditional American))  dietary_preference (Main dietary restriction or lifestyle the user follows (e.g., Vegetarian, Low-carb, No restrictions))  food_preferences (Preferred food categories and meal types (e.g., comfort food, healthy salads, baked goods, grilled meats))  top_cuisine_choices (Specific regional or ethnic cuisines the user frequently rates highly (e.g., Thai, Southern BBQ, French pastry))  dietary_preferences (Dietary restrictions, health considerations, or eating patterns (e.g., gluten-free, plant-based, high-protein, dairy-free))  flavor_preferences (Dominant taste profiles and flavor characteristics the user seeks (e.g., bold and spicy, mild and creamy, tangy and citrusy))  daypart_preferences (Preferred times of day for different meal types based on rating patterns (e.g., hearty breakfast, light lunch, elaborate dinner))  lifestyle_tags (Behavioral patterns and cooking style indicators inferred from recipe choices (e.g., quick meals, entertainer, health-conscious, experimental cook))  convenience_preference (Preference for recipe complexity (e.g., quick and easy, gourmet elaborate))  diversity_openness (Willingness to try new cuisines (e.g., adventurous, selective, traditionalist, not defined))  notes (Brief summary explaining the users overall food personality and any notable patterns in their preferences)  justification (Detailed explanation of how the profile was determined based on the users interaction history and ratings. Describe why the liked cuisines, cuisine preference, dietary preference, food preferences, cuisine preferences, dietary preferences, flavor preferences, daypart preferences, and lifestyle tags were chosen. Is not allowed to use quotes or complex punctuation in this field. Keep it between 100 and 200 words not more.)  user_story (Predictive narrative about the user s culinary evolution and potential future preferences. Describes their food journey, emerging patterns, and likely directions for taste exploration. Written to help predict what they might enjoy next based on their current trajectory and evolving palate.)  future_preferences (Speculative insights into the types of recipes and cuisines the user may be inclined to explore in the future. Based on their current preferences, suggest new food categories, cooking styles, or dietary trends they might be open to trying next. This helps in anticipating their evolving culinary interests.)  ]. Each fill of the structured output doesnt need to take more than 200 words keep it in mind. IMPORTANT: Do not use quotation marks or complex punctuation in your response. Use simple words and avoid any quotes, apostrophes, or special characters. Use the following interaction history as reference:', s.history_string),\n",
      "        connection_id => 'us.kaggle-connection',\n",
      "        endpoint => 'gemini-2.5-flash',\n",
      "        model_params => JSON '{\"generationConfig\":{\"temperature\": 1.0, \"maxOutputTokens\": 2048, \"thinking_config\": {\"thinking_budget\": 1024} } }',\n",
      "        output_schema => 'liked_cuisines ARRAY<STRING>, cuisine_preference STRING, dietary_preference STRING, food_preferences ARRAY<STRING>, top_cuisine_choices ARRAY<STRING>, dietary_preferences ARRAY<STRING>, flavor_preferences ARRAY<STRING>, daypart_preferences ARRAY<STRING>, lifestyle_tags ARRAY<STRING>, convenience_preference STRING, diversity_openness STRING, notes STRING, justification STRING, user_story STRING, future_preferences STRING'\n",
      "    ) AS ai_result\n",
      "  FROM (SELECT * FROM `deliverable.users_parsed`) s\n",
      ")\n",
      "SELECT \n",
      "  *,\n",
      "  ai_result.full_response AS user_profile,\n",
      "  JSON_EXTRACT_SCALAR(ai_result.full_response, '$.candidates[0].content.parts[0].text') AS user_profile_text\n",
      "FROM ai_responses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_profile_generation_query = f\"\"\"\n",
    "WITH ai_responses AS (\n",
    "  SELECT \n",
    "    s.user_id, \n",
    "    s.n_history,\n",
    "    s.history_string,\n",
    "    AI.GENERATE(('{user_profile_prompt}', s.history_string),\n",
    "        connection_id => '{CONNECTION_ID}',\n",
    "        endpoint => 'gemini-2.5-flash',\n",
    "        model_params => JSON '{{\"generationConfig\":{{\"temperature\": 1.0, \"maxOutputTokens\": 2048, \"thinking_config\": {{\"thinking_budget\": 1024}} }} }}',\n",
    "        output_schema => 'liked_cuisines ARRAY<STRING>, cuisine_preference STRING, dietary_preference STRING, food_preferences ARRAY<STRING>, top_cuisine_choices ARRAY<STRING>, dietary_preferences ARRAY<STRING>, flavor_preferences ARRAY<STRING>, daypart_preferences ARRAY<STRING>, lifestyle_tags ARRAY<STRING>, convenience_preference STRING, diversity_openness STRING, notes STRING, justification STRING, user_story STRING, future_preferences STRING'\n",
    "    ) AS ai_result\n",
    "  FROM (SELECT * FROM `{USERS_PARSED}`) s\n",
    ")\n",
    "SELECT \n",
    "  *,\n",
    "  ai_result.full_response AS user_profile,\n",
    "  JSON_EXTRACT_SCALAR(ai_result.full_response, '$.candidates[0].content.parts[0].text') AS user_profile_text\n",
    "FROM ai_responses\n",
    "\"\"\"\n",
    "\n",
    "print(user_profile_generation_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1244d8",
   "metadata": {},
   "source": [
    "# Vector Search vs ALS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2585d06",
   "metadata": {},
   "source": [
    "We begin creating the text embeddings for both users and recipes, as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34726e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.query_and_wait(f\"\"\"\n",
    "ALTER TABLE `{PROJECT_ID}.{USERS_PROFILES_TABLE}`\n",
    "ADD COLUMN text_embedding ARRAY<FLOAT64>\n",
    "\"\"\")\n",
    "\n",
    "client.query_and_wait(f\"\"\"\n",
    "UPDATE `{PROJECT_ID}.{USERS_PROFILES_TABLE}` AS t\n",
    "SET t.text_embedding = s.ml_generate_embedding_result\n",
    "FROM (\n",
    "  SELECT\n",
    "    user_id,\n",
    "    ml_generate_embedding_result\n",
    "  FROM\n",
    "    ML.GENERATE_EMBEDDING(\n",
    "      MODEL `{SCHEMA_NAME}.text_embedding_model`,\n",
    "      (\n",
    "        SELECT\n",
    "          user_id,\n",
    "          user_profile_text AS content\n",
    "        FROM `{PROJECT_ID}.{USERS_PROFILES_TABLE}`\n",
    "      ),\n",
    "      STRUCT(TRUE AS flatten_json_output, {OUT_DIM} AS OUTPUT_DIMENSIONALITY, 'RETRIEVAL_QUERY' AS task_type)\n",
    "    )\n",
    ") AS s\n",
    "WHERE t.user_id = s.user_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999d2c9",
   "metadata": {},
   "source": [
    "We could then reduce the search space of the vector search query using business logic. For example, we could filter recipes by cuisine, meal type, or dietary restrictions based on the user's preferences. This would help in retrieving more relevant recommendations. But, it is out of the scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f38dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "754a6549",
   "metadata": {},
   "source": [
    "## Naive Approach (Collaborative Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973fc386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3adda804",
   "metadata": {},
   "source": [
    "## Simple Vector Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f88f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b22ec48",
   "metadata": {},
   "source": [
    "## HyDE (Hypothetical Document Embedding) Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6a220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "429c05b4",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3a2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a034900",
   "metadata": {},
   "source": [
    "We see in other competitions like [H&M](https://www.kaggle.com/code/julian3833/h-m-implicit-als-model-0-014) and in papers like ...\n",
    "\n",
    "If we'd had a more reliable retriever (powered by any algorithm, heuristic or methodology), we could have used `AI.GENERATE` method to re-rank the retrieved candidates as in [LlamaRec](https://arxiv.org/pdf/2311.02089). Some RecSys ideas can be found in more advanced papers like [LRU](https://arxiv.org/pdf/2310.02367)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0bd36",
   "metadata": {},
   "source": [
    "# Hypothesis (Explanation) Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3c27f",
   "metadata": {},
   "source": [
    "Inspired by the work of [Spotify](https://arxiv.org/abs/2508.08777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b82f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0caf7d69",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge as middle ground between Offline Metrics and A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2026f14",
   "metadata": {},
   "source": [
    "Following the ideas of this [Spotify paper](https://dl.acm.org/doi/pdf/10.1145/3705328.3759305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e1a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06532c41",
   "metadata": {},
   "source": [
    "# UI with Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711751a",
   "metadata": {},
   "source": [
    "# Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65980f1c",
   "metadata": {},
   "source": [
    "## Feedback on BigQuery AI features\n",
    "- We found difficult at the beggining to pass the description of the attributes of the Pydantic models (used in libraries like LangChain and Instructor) as schema for the LLM calls. As you see in the code, we had to create a function to create a part of the prompt with the description of each attribute. It would be great to have a more straightforward way to do this.\n",
    "- The LLM calls are quite slow, even using the lightweight models. We had to limit the number of recipes and users to process. It would be great to have a way to speed up the calls, and show the progress of the calls.\n",
    "- If a response is too lengthy and surpasses the maximum tokens limit, it may cause errors related to quotation marks. Improving error messages and handling for these cases would make debugging easier and prevent the entire query from failing due to such edge cases.\n",
    "- We found that sometimes the Gemini Embedding 001 model returns empty embeddings for certain inputs. ...\n",
    "\n",
    "## User Survey on BigQuery AI features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5988be1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
