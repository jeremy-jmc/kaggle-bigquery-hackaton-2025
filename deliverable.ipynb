{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ae122a",
   "metadata": {},
   "source": [
    "- **Project Title:** *Explainable Recommendations with BigQuery AI: From Profiles to Insights*\n",
    "- **Problem Statement:** Most recommendation systems function as “black boxes,” surfacing items without offering context or transparency. This project uses BigQuery AI to automatically generate structured profiles of users and items, transform them into embeddings, and apply vector search to retrieve the most relevant matches. Each recommendation is then paired with an LLM-generated explanation, providing narrative reasoning that makes the system’s suggestions interpretable and trustworthy.\n",
    "- **Impact Statement:** This solution shows how organizations can build scalable, explainable recommendation engines directly within BigQuery without the need for specialized machine learning teams. By combining personalization with transparency, the approach improves user trust, helps businesses uncover actionable behavioral patterns, and extends seamlessly to domains such as e-commerce and media.\n",
    "- **Project Highlights**\n",
    "    - Developed adaptable, structured profiles for users and recipes using BigQuery AI, enabling easy customization for various business domains.\n",
    "    - Demonstrated that a straightforward recommendation system powered by BigQuery AI can slightly outperform the ALS baseline, simplifying implementation and highlighting the value of LLMs in recommendation workflows.\n",
    "    - Created an interactive UI for users to browse personalized recommendations alongside clear, LLM-generated explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622911ca",
   "metadata": {},
   "source": [
    "# User & Recipe Profiles with BigQuery AI: From Embeddings to \"Explainable\" Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9356df23",
   "metadata": {},
   "source": [
    "\n",
    "Inspired by the DoorDash blog post on [Profile Generation with LLMs](https://careersatdoordash.com/blog/doordash-profile-generation-llms-understanding-consumers-merchants-and-items/), this notebook showcases how to build user and recipe profiles leveraging BigQuery AI. It also outlines a workflow for generating potential recommendations, each paired with an \"LLM-generated hypothesis\" to provide narrative context for the suggestions.\n",
    "\n",
    "This project addresses the challenges of **Approach 1 (The AI Architect)** and **Approach 2 (The Semantic Detective)** as described in the [Kaggle BigQuery Hackathon 2025](https://www.kaggle.com/competitions/bigquery-ai-hackathon/overview).\n",
    "\n",
    "This notebook is structured as follows:\n",
    "\n",
    "1. Data Sources\n",
    "    - Datasets used (including sources and descriptions)\n",
    "    - Preprocessing, splitting and uploading to BigQuery\n",
    "\n",
    "2. (Approach 1) Recipe Profiles Generation\n",
    "    - Profile schema design & prompt engineering\n",
    "    - Using BigQuery AI to generate recipe profiles\n",
    "\n",
    "3. (Approach 1) User Profiles Generation\n",
    "    - Profile schema design & prompt engineering\n",
    "    - Using BigQuery AI to generate user profiles\n",
    "\n",
    "4. (Approach 2) Vector Search vs ALS\n",
    "    - Using BigQuery AI to generate text embeddings\n",
    "    - Simple collaborative filtering\n",
    "    - Vector search for recommendations\n",
    "        - Naive approach\n",
    "        - HyDE (Hypothetical Document Embedding) approach\n",
    "    - Comparing results with ALS baseline\n",
    "\n",
    "5. (Approach 1) Explanation Generation\n",
    "    - Prompt engineering for explanation generation\n",
    "    - Using BigQuery AI to generate explanations for recommendations\n",
    "\n",
    "6. (Approach 2) UI with Streamlit \n",
    "    - Hands-on interface to browse recommendations and their rationales\n",
    "        - Automatic Personalized Recommendations\n",
    "        - Semantic Search of Recipes\n",
    "\n",
    "7. Summary and Conclusions\n",
    "    - Feedback on BigQuery AI features\n",
    "    - User Survey on BigQuery AI features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595f34d",
   "metadata": {},
   "source": [
    "## Setup & Project Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os, json\n",
    "os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "import os, ast, numpy as np, pandas as pd\n",
    "import random\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "import bigframes.pandas as bpd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJECT_ID'] = input(\"Enter your Google Cloud Project ID: \")\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "\n",
    "subprocess.run(['gcloud', 'auth', 'login'])\n",
    "subprocess.run(['gcloud', 'config', 'set', 'project', PROJECT_ID])\n",
    "subprocess.run(['gcloud', 'auth', 'application-default', 'set-quota-project', PROJECT_ID])\n",
    "\n",
    "bpd.options.bigquery.project = PROJECT_ID\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "CONNECTION_ID = 'us.kaggle-connection'\n",
    "SCHEMA_NAME = 'foodrecsys'\n",
    "VALID_INTERACTIONS = f\"{PROJECT_ID}.{SCHEMA_NAME}.valid_interactions_windowed\"\n",
    "TRAIN_INTERACTIONS = f\"{PROJECT_ID}.{SCHEMA_NAME}.train_interactions_windowed\"\n",
    "SUBSET_RECIPE_IDS = f\"{PROJECT_ID}.{SCHEMA_NAME}.final_recipes\"\n",
    "SUBSET_USERS_IDS = f\"{PROJECT_ID}.{SCHEMA_NAME}.final_users\"\n",
    "\n",
    "RECIPES_ALL = f\"{PROJECT_ID}.{SCHEMA_NAME}.recipes\"\n",
    "OUT_DIM = 1024\n",
    "\n",
    "RECIPES_PARSED = f'{SCHEMA_NAME}.recipes_parsed'\n",
    "RECIPES_PROFILES_TABLE = f\"{SCHEMA_NAME}.recipe_profiles\"\n",
    "\n",
    "USERS_PARSED = f'{SCHEMA_NAME}.users_parsed'\n",
    "USERS_PROFILES_TABLE = f\"{SCHEMA_NAME}.user_profiles\"\n",
    "\n",
    "VECTOR_SEARCH_RESULTS_TABLE = f\"{SCHEMA_NAME}.vector_search_results\"\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce90e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_to_prompt_with_descriptions(model_class) -> str:\n",
    "    prompt = \"\"\n",
    "    for k, v in model_class.model_json_schema()['properties'].items():\n",
    "        desc = v.get('description', '')\n",
    "        prompt += f\" {k} ({desc}) \"\n",
    "    return f\"[ {prompt} ]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b9457",
   "metadata": {},
   "source": [
    "Following the tutorials on [GitHub](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb) we create a remote connection to the Text Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acbae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.query_and_wait(f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{SCHEMA_NAME}.text_embedding_model`\n",
    "REMOTE WITH \n",
    "    CONNECTION `{CONNECTION_ID}`\n",
    "    OPTIONS (ENDPOINT = 'gemini-embedding-001');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28699f8",
   "metadata": {},
   "source": [
    "# Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179a96d",
   "metadata": {},
   "source": [
    "For the dataset selection, three considerations were made:\n",
    "- The datasets are publicly available (in line with the competition guidelines)\n",
    "- The dataset should align with the goal of addressing a real-world problem using solutions that integrate naturally with SQL workflows.\n",
    "- The dataset needs to include unstructured, messy, and mixed data\n",
    "\n",
    "\n",
    "We utilize the [FoodRecSysV1](https://www.kaggle.com/datasets/elisaxxygao/foodrecsysv1) dataset from Kaggle, which provides comprehensive user interactions with recipes; including ratings, comments, and rich recipe metadata. This dataset closely mirrors the real-world challenges encountered by platforms such as Cookpad, DoorDash, UberEats, and Rappi.\n",
    "\n",
    "Other datasets like [MealRec](https://arxiv.org/abs/2205.12133) and [MealRec++](https://github.com/WUT-IDEA/MealRecPlus) were also considered (and we can apply the same methodology of this notebook), but for the sake of simplicity and ease to start, we opted for FoodRecSysV1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Read the dataset and reproduce the results into a new BigQuery dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59dd96f",
   "metadata": {},
   "source": [
    "We'll finally use a total of ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24b5bb",
   "metadata": {},
   "source": [
    "# Recipe Profiles Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf54445",
   "metadata": {},
   "source": [
    "We start cleaning and parsing some columns to make them suitable and more understandable for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb42144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53e35ffb",
   "metadata": {},
   "source": [
    "Following the [DoorDash blog](https://careersatdoordash.com/blog/doordash-profile-generation-llms-understanding-consumers-merchants-and-items/) we define the structure and attributes of the \"Recipe Profile\" (item profiles). In this case we use a Pydantic model to define the schema, which then parsed as part of the prompt using the function `schema_to_prompt_with_descriptions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b85692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeProfile(BaseModel):\n",
    "    food_type: str = Field(description=\"Type of food, e.g., dessert, main course, appetizer\")\n",
    "    cuisine_type: str = Field(description=\"Cuisine type, e.g., Italian, Chinese, Mexican, American\")\n",
    "    dietary_preferences: List[str] = Field(description=\"Dietary preferences, e.g., omnivore, vegetarian, vegan, gluten-free\")\n",
    "    flavor_profile: List[str] = Field(description=\"Flavor profile, e.g., spicy, sweet, savory\")\n",
    "    serving_daypart: List[str] = Field(description=\"Suitable dayparts, e.g., breakfast, lunch, dinner\")\n",
    "    notes: str = Field(description=\"Short rationale summarizing the recipe profile\")\n",
    "    target_audience: str = Field(description=\"Types of users who would likely enjoy this recipe based on cooking skill level, flavor intensity, dietary needs, and lifestyle preferences. Helps recommendation systems match recipes to appropriate user profiles.\")\n",
    "    justification: str = Field(description=\"Detailed explanation of how the profile was determined Describe why the food type, cuisine type, dietary preferences, flavor profile, and serving daypart were chosen based on the ingredients and cooking directions. Is not allowed to use quotes or complex punctuation in this field.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fa17e",
   "metadata": {},
   "source": [
    "This schema could be enhanced by using more restrictive field types, such as `Enums` or `Literal` for categorical attributes. This would make it easier to apply business logic based on the extracted category metadata generated by the LLM in future iterations. \n",
    "\n",
    "Then, we define the prompt template to generate the recipe profiles, as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744fd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_profile_prompt = f\"\"\"Based on the title, ingredients, cooking directions and percent daily values provided, create a recipe profile that summarizes the key characteristics of this recipe. Your response must follow this exact structure: {schema_to_prompt_with_descriptions(RecipeProfile)}. IMPORTANT: Do not use quotation marks or complex punctuation in your response. Use simple words and avoid any quotes, apostrophes, or special characters.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_profile_generation_query = f\"\"\"\n",
    "WITH ai_responses AS (\n",
    "  SELECT \n",
    "    s.recipe_id, \n",
    "    s.title, \n",
    "    s.ingredients, \n",
    "    s.cooking_directions, \n",
    "    s.nutritions, \n",
    "    s.reviews, \n",
    "    s.parsed_ingredients, \n",
    "    s.parsed_recipe,\n",
    "    AI.GENERATE(('{recipe_profile_prompt}', s.parsed_ingredients, s.parsed_recipe, s.percent_daily_values),\n",
    "        connection_id => '{CONNECTION_ID}',\n",
    "        endpoint => 'gemini-2.5-flash',\n",
    "        model_params => JSON '{{\"generationConfig\":{{\"temperature\": 1.0, \"maxOutputTokens\": 2048, \"thinking_config\": {{\"thinking_budget\": 1024}} }} }}',\n",
    "        output_schema => 'food_type STRING, cuisine_type STRING, dietary_preferences ARRAY<STRING>, flavor_profile ARRAY<STRING>, serving_daypart ARRAY<STRING>, notes STRING, target_audience STRING, justification STRING'\n",
    "    ) AS ai_result\n",
    "  FROM (SELECT * FROM `{RECIPES_PARSED}`) s\n",
    ")\n",
    "SELECT \n",
    "  *,\n",
    "  ai_result.full_response AS recipe_profile,\n",
    "  JSON_EXTRACT_SCALAR(ai_result.full_response, '$.candidates[0].content.parts[0].text') AS recipe_profile_text\n",
    "FROM ai_responses\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095acf73",
   "metadata": {},
   "source": [
    "# User Profiles Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ec940",
   "metadata": {},
   "source": [
    "For the user profile generation, we take advantage of the context window size of the Gemini models to provide a list of their historical interactions (ratings and comments) as part of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_history(user_id: int, n: int = 25, k_min: int = 5) -> list:\n",
    "    pass\n",
    "\n",
    "def format_user_history(user_history: list[dict]) -> str:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51563b",
   "metadata": {},
   "source": [
    "As in the recipe profiles, we define a Pydantic model, that it can be improved and adapted to the specific business case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64122a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserProfile(BaseModel):\n",
    "    liked_cuisines: List[str] = Field(description=\"List of cuisines the user enjoys most, ranked by preference based on their interaction history and ratings\")\n",
    "    cuisine_preference: str = Field(description=\"Primary cuisine type the user gravitates towards (e.g., Mediterranean, Asian Fusion, Traditional American)\")\n",
    "    dietary_preference: str = Field(description=\"Main dietary restriction or lifestyle the user follows (e.g., Vegetarian, Low-carb, No restrictions)\")\n",
    "\n",
    "    food_preferences: List[str] = Field(description=\"Preferred food categories and meal types (e.g., comfort food, healthy salads, baked goods, grilled meats)\")\n",
    "    top_cuisine_choices: List[str] = Field(description=\"Specific regional or ethnic cuisines the user frequently rates highly (e.g., Thai, Southern BBQ, French pastry)\")\n",
    "    dietary_preferences: List[str] = Field(description=\"Dietary restrictions, health considerations, or eating patterns (e.g., gluten-free, plant-based, high-protein, dairy-free)\")\n",
    "    flavor_preferences: List[str] = Field(description=\"Dominant taste profiles and flavor characteristics the user seeks (e.g., bold and spicy, mild and creamy, tangy and citrusy)\")\n",
    "    daypart_preferences: List[str] = Field(description=\"Preferred times of day for different meal types based on rating patterns (e.g., hearty breakfast, light lunch, elaborate dinner)\")\n",
    "    lifestyle_tags: List[str] = Field(description=\"Behavioral patterns and cooking style indicators inferred from recipe choices (e.g., quick meals, entertainer, health-conscious, experimental cook)\")\n",
    "    convenience_preference: str = Field(description=\"Preference for recipe complexity (e.g., quick and easy, gourmet elaborate)\")\n",
    "    diversity_openness: str = Field(description=\"Willingness to try new cuisines (e.g., adventurous, selective, traditionalist, not defined)\")\n",
    "\n",
    "    notes: str = Field(description=\"Brief summary explaining the users overall food personality and any notable patterns in their preferences\")\n",
    "    justification: str = Field(description=\"Detailed explanation of how the profile was determined based on the users interaction history and ratings. Describe why the liked cuisines, cuisine preference, dietary preference, food preferences, cuisine preferences, dietary preferences, flavor preferences, daypart preferences, and lifestyle tags were chosen. Is not allowed to use quotes or complex punctuation in this field. Keep it between 100 and 200 words not more.\")\n",
    "    user_story: str = Field(description=\"Predictive narrative about the user s culinary evolution and potential future preferences. Describes their food journey, emerging patterns, and likely directions for taste exploration. Written to help predict what they might enjoy next based on their current trajectory and evolving palate.\")\n",
    "    future_preferences: str = Field(description=\"Speculative insights into the types of recipes and cuisines the user may be inclined to explore in the future. Based on their current preferences, suggest new food categories, cooking styles, or dietary trends they might be open to trying next. This helps in anticipating their evolving culinary interests.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e46f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile_prompt = f\"\"\"Generate a structured user profile that captures their culinary tastes, dietary preferences, flavor inclinations, among others. This user profile will be used then for a Recommendation System. Ensure the profile is concise, reasonable and accurately reflects the users food personality based on their interaction history. Please provide a structured profile of the user using the following format: {schema_to_prompt_with_descriptions(UserProfile)}. Each fill of the structured output doesnt need to take more than 200 words keep it in mind. IMPORTANT: Do not use quotation marks or complex punctuation in your response. Use simple words and avoid any quotes, apostrophes, or special characters. Use the following interaction history as reference:\"\"\"\n",
    "\n",
    "user_profile_generation_query = f\"\"\"\n",
    "WITH ai_responses AS (\n",
    "  SELECT \n",
    "    s.user_id, \n",
    "    s.n_history,\n",
    "    s.history_string,\n",
    "    AI.GENERATE(('{user_profile_prompt}', s.history_string),\n",
    "        connection_id => '{CONNECTION_ID}',\n",
    "        endpoint => 'gemini-2.5-flash',\n",
    "        model_params => JSON '{{\"generationConfig\":{{\"temperature\": 1.0, \"maxOutputTokens\": 2048, \"thinking_config\": {{\"thinking_budget\": 1024}} }} }}',\n",
    "        output_schema => 'liked_cuisines ARRAY<STRING>, cuisine_preference STRING, dietary_preference STRING, food_preferences ARRAY<STRING>, top_cuisine_choices ARRAY<STRING>, dietary_preferences ARRAY<STRING>, flavor_preferences ARRAY<STRING>, daypart_preferences ARRAY<STRING>, lifestyle_tags ARRAY<STRING>, convenience_preference STRING, diversity_openness STRING, notes STRING, justification STRING, user_story STRING, future_preferences STRING'\n",
    "    ) AS ai_result\n",
    "  FROM (SELECT * FROM `{USERS_PARSED}`) s\n",
    ")\n",
    "SELECT \n",
    "  *,\n",
    "  ai_result.full_response AS user_profile,\n",
    "  JSON_EXTRACT_SCALAR(ai_result.full_response, '$.candidates[0].content.parts[0].text') AS user_profile_text\n",
    "FROM ai_responses\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1244d8",
   "metadata": {},
   "source": [
    "# Vector Search vs ALS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2585d06",
   "metadata": {},
   "source": [
    "We begin creating the text embeddings for both users and recipes, as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34726e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.query_and_wait(f\"\"\"\n",
    "ALTER TABLE `{PROJECT_ID}.{USERS_PROFILES_TABLE}`\n",
    "ADD COLUMN text_embedding ARRAY<FLOAT64>\n",
    "\"\"\")\n",
    "\n",
    "client.query_and_wait(f\"\"\"\n",
    "UPDATE `{PROJECT_ID}.{USERS_PROFILES_TABLE}` AS t\n",
    "SET t.text_embedding = s.ml_generate_embedding_result\n",
    "FROM (\n",
    "  SELECT\n",
    "    user_id,\n",
    "    ml_generate_embedding_result\n",
    "  FROM\n",
    "    ML.GENERATE_EMBEDDING(\n",
    "      MODEL `{SCHEMA_NAME}.text_embedding_model`,\n",
    "      (\n",
    "        SELECT\n",
    "          user_id,\n",
    "          user_profile_text AS content\n",
    "        FROM `{PROJECT_ID}.{USERS_PROFILES_TABLE}`\n",
    "      ),\n",
    "      STRUCT(TRUE AS flatten_json_output, {OUT_DIM} AS OUTPUT_DIMENSIONALITY, 'RETRIEVAL_QUERY' AS task_type)\n",
    "    )\n",
    ") AS s\n",
    "WHERE t.user_id = s.user_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999d2c9",
   "metadata": {},
   "source": [
    "We could then reduce the search space of the vector search query using business logic. For example, we could filter recipes by cuisine, meal type, or dietary restrictions based on the user's preferences. This would help in retrieving more relevant recommendations. But, it is out of the scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f38dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "754a6549",
   "metadata": {},
   "source": [
    "## Naive Approach (Collaborative Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973fc386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3adda804",
   "metadata": {},
   "source": [
    "## Simple Vector Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f88f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b22ec48",
   "metadata": {},
   "source": [
    "## HyDE (Hypothetical Document Embedding) Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6a220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "429c05b4",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3a2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a034900",
   "metadata": {},
   "source": [
    "We see in other competitions like [H&M](https://www.kaggle.com/code/julian3833/h-m-implicit-als-model-0-014) and in papers like ...\n",
    "\n",
    "If we'd had a more reliable retriever (powered by any algorithm, heuristic or methodology), we could have used `AI.GENERATE` method to re-rank the retrieved candidates as in [LlamaRec](https://arxiv.org/pdf/2311.02089). Some RecSys ideas can be found in more advanced papers like [LRU](https://arxiv.org/pdf/2310.02367)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0bd36",
   "metadata": {},
   "source": [
    "# Hypothesis (Explanation) Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3c27f",
   "metadata": {},
   "source": [
    "Inspired by the work of [Spotify](https://arxiv.org/abs/2508.08777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b82f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06532c41",
   "metadata": {},
   "source": [
    "# UI with Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711751a",
   "metadata": {},
   "source": [
    "# Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65980f1c",
   "metadata": {},
   "source": [
    "## Feedback on BigQuery AI features\n",
    "- We found difficult at the beggining to pass the description of the attributes of the Pydantic models (used in libraries like LangChain and Instructor) as schema for the LLM calls. As you see in the code, we had to create a function to create a part of the prompt with the description of each attribute. It would be great to have a more straightforward way to do this.\n",
    "- The LLM calls are quite slow, even using the lightweight models. We had to limit the number of recipes and users to process. It would be great to have a way to speed up the calls, and show the progress of the calls.\n",
    "- If a response is too lengthy and surpasses the maximum tokens limit, it may cause errors related to quotation marks. Improving error messages and handling for these cases would make debugging easier and prevent the entire query from failing due to such edge cases.\n",
    "- We found that sometimes the Gemini Embedding 001 model returns empty embeddings for certain inputs. ...\n",
    "\n",
    "## User Survey on BigQuery AI features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5988be1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
